{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis\n",
    "   \n",
    "   <p>All requested statistics for the Boston Housing dataset are accurately calculated. Student correctly leverages NumPy functionality to obtain these results.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('housing.csv')\n",
    "prices = data['MEDV']\n",
    "features = data.drop('MEDV', axis = 1)\n",
    "\n",
    "print(\"Boston housing dataset has {} datapoints with {} variables each\".format(*data.shape))\n",
    "\n",
    "min_price = np.min(prices)\n",
    "print(\"Min:\"+\"\\n\"+str(min_price))\n",
    "\n",
    "max_price = np.max(prices)\n",
    "print(\"Max:\"+\"\\n\"+str(max_price))\n",
    "\n",
    "median_price = np.median(prices)\n",
    "print(\"Median:\"+\"\\n\"+str(median_price))\n",
    "\n",
    "std_price = np.std(prices)\n",
    "print(\"Standard Deviation:\"+\"\\n\"+str(std_price))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Feature Observation\n",
    "\n",
    "   \n",
    "   <p>\n",
    "    'RM' is the average number of rooms among homes in the neighborhood.\n",
    "    'LSTAT' is the percentage of homeowners in the neighborhood considered \"lower class\" (working poor). \n",
    "    'PTRATIO' is the ratio of students to teachers in primary and secondary schools in the neighborhood.\n",
    "    \n",
    "    Using your intuition, for each of the three features above, do you think that an increase in the value of that feature would lead to an increase in the value of 'MEDV' or a decrease in the value of 'MEDV'? Justify your answer for each.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer:\n",
    "\n",
    "   <p>Independent variables:\n",
    "    1) RM: Number of rooms in a house\n",
    "    2) LSTAT: Percentage of neighborhood population below poverty line.\n",
    "    3) PTR: Pupil-Teacher ratio\n",
    "    \n",
    "    Dependent variable:\n",
    "    1) Price of the house\n",
    "    \n",
    "    Correlations:\n",
    "    1) RM: Positively correlated to price. As RM goes up, price goes up.\n",
    "    2) LSTAT: Negatively correlated to price. As LSTAT goes up, price goes down.\n",
    "    3) PTR: Negatively correlated to price. As PTR goes up, price goes down.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - Goodness of Fit\n",
    "   \n",
    "   <p>\n",
    "    Assume that a dataset contains five data points and a model made the following predictions for the target variable:\n",
    "    <br>\n",
    "    <br>\n",
    "    True Values:[3.0, -0.5, 2.0, 7.0, 4.2]<br>\t\n",
    "    Predictions:[2.5, 0.0, 2.1, 7.8, 5.3]<br><br>\n",
    "        \n",
    "    Run the code cell below to use the performance_metric function and calculate this model's coefficient of determination.\n",
    "  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the performance of this model\n",
    "score = performance_metric([3, -0.5, 2.7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\n",
    "print \"Model has a coefficient of determination, R^2, of {:.3f}.\".format(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "   <p>Would you consider this model to have successfully captured the variation of the target variable? Why or why not?</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer:\n",
    "\n",
    "   <p>The r2_score function was imported from sklearn.metrics and the arrays of true and predicted values were plugged in. The resulting value calculated was 0.923.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "def performance_metric(y_true, y_predict):\n",
    "\terror = r2_score(y_true, y_predict)\n",
    "\treturn error\n",
    "print(\"r2_score\"+\"\\n\"+str(performance_metric(y_train, y_train)))\n",
    "print(str(performance_metric([3, -0.5, 2.7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])))\n",
    ">>> 0.923"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   <p>I would say this metric is pretty successful in capturating the variation of the target variable for this data. The fairly high value of 0.923 indicates a fairly good correlation between the true and predicted values. While the predictions are off by about 0.1-1.0, the predictions are consistently higher for the higher values and lower for the lower values.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3:\n",
    "   \n",
    "   <p>What is the benefit to splitting a dataset into some ratio of training and testing subsets for a learning algorithm?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer:\n",
    "   \n",
    "   <p>Saving a portion of the dataset for testing allows us to evaluate the strength of our model and determine if we are overfitting or underfitting. If the model works well for the training data but poorly for the testing data, then the model overfits. If the model works well on the testing data but poorly on the training data, then the model underfits. \n",
    "    \n",
    "    Below, the code for splitting the dataset is shown: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "#Obtain the data\n",
    "data = pd.read_csv('housing.csv')\n",
    "prices = data['MEDV']\n",
    "features = data.drop('MEDV', axis = 1)\n",
    "#Split the data\n",
    "X_train, X_test, y_train, t_test = train_test_split(prices,\n",
    "\t\t\t\t\t\tfeatures,\n",
    "\t\t\t\t\t\ttest_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4:\n",
    "   \n",
    "   <p>Choose one of the graphs above and state the maximum depth for the model.\n",
    "What happens to the score of the training curve as more training points are added? What about the testing curve?\n",
    "Would having more training points benefit the model?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "   \n",
    "   <p>Below, the learning curves prodcuded by the line 'vs.ModelLearning(features, prices)' in boston_housing.py are shown.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "   <p>Based on this these images, the best fit is the model with max_depth=3, because the score for the testing and training sets converge at a fairly high level. 150 data points is enough for the scores for the two sets to be nearly converged, and beyond 300 data points, the model does improve significantly.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "   \n",
    "   <p>When the model is trained with a maximum depth of 1, does the model suffer from high bias or from high variance?\n",
    "How about when the model is trained with a maximum depth of 10? What visual cues in the graph justify your conclusions?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "\n",
    "   <p>The model with max_depth 10 always has a high score for the cross validation data, but never gets very good for the training data no matter how many data points are provided. This is a model with high variance/overfitting.\n",
    "    \n",
    "    The model with a max_depth of 1 never has a very high score for either the training or testing sets, but the two score values converge fairly quickly. This is a model with high bias/underfitting.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "   \n",
    "   <p>Which maximum depth do you think results in a model that best generalizes to unseen data?\n",
    "What intuition lead you to this answer?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer:\n",
    "   \n",
    "   <p>The best model appears to be the model with max_depth=3, because the scores for the training and testing sets seem to converge at a fairly high value, using a reasonable number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "   \n",
    "   <p>What is the grid search technique? How it can be applied to optimize a learning algorithm?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "    \n",
    "   <p>The grid search technique tests a range of models against the data set, the models varying by two or more parameters (e.g. polynomial degree, number of data points in training set, etc.), which can be imagined as axes on a grid. Each point on the grid represents a model. Each point on the grid is given a score, and the model at the best scoring point is the optimal model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "   <p>What is the k-fold cross-validation training technique?\n",
    "What benefit does this technique provide for grid search when optimizing a model?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "   <p>The draw back to splitting the data set into distinct training and testing sets is that some of the data goes to waste during testing and training. With k-fold cross validation, we instead split the data points into a number (k) buckets. The model is trained k times, and each time the testing set is different. This way, we get to use all the data for testing and training.The value of k is a parameter which can be optimized in a grid search</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9\n",
    "   <p>What maximum depth does the optimal model have? How does this result compare to your guess in Question 6?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "   <p>The code for my fit_model function is shown below: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X, y):\n",
    "        regressor = DecisionTreeRegressor()\n",
    "        parameters = {'max_depth':(1,2,3,4,5,6,7,8,9)}\n",
    "        scoring_function = make_scorer(performance_metric,\n",
    "         greater_is_better=False)\n",
    "        reg = GridSearchCV(regressor, parameters, scoring=scoring_function)\n",
    "        reg.fit(X,y)\n",
    "        print str(reg.best_estimator_)\n",
    "        return reg.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><p>The output of <code>print str(reg.best_estimator_)</code> is shown below</p><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeRegressor(criterion='mse', max_depth=4, max_features=None,\n",
    "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "           min_impurity_split=None, min_samples_leaf=1,\n",
    "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "           presort=False, random_state=None, splitter='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <p>The max_depth for this function is 4. This is close to my best guess of 3 from the learning curves.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10\n",
    "   <p> Imagine that you were a real estate agent in the Boston area looking to use this model to help price homes owned by your clients that they wish to sell. You have collected the following information from three of your clients:\n",
    "<table>    \n",
    "    <tr>\n",
    "        <td>Feature</td><td>Client 1</td><td>Client 2</td><td>Client 3</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>total number of rooms in home</td><td>5</td><td>4</td><td>8</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Neighborhood poverty (as %)</td>\n",
    "        <td>17%</td>\n",
    "        <td>32%</td>\n",
    "        <td>3%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Student-teacher ratio</td>\n",
    "        <td>15-to-1</td>\n",
    "        <td>22-to-1</td>\n",
    "        <td>12-to-1</td>\n",
    "    </tr>\n",
    "\t\t    \t    \n",
    "</table>\n",
    "<br>\n",
    "What price would you recommend each client sell his/her home at?\n",
    "\n",
    "<br>\n",
    "Do these prices seem reasonable given the values for the respective features?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "   <p>The code for predicting the home values is shown below:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "client_data = [[5,17,15],[4,32,22],[8,3,12]]\n",
    "\n",
    "data = pd.read_csv('housing.csv')\n",
    "prices = data['MEDV']\n",
    "features = data.drop('MEDV', axis = 1)\n",
    "\n",
    "def performance_metric(y_true, y_predict):\n",
    "        r2 = r2_score(y_true, y_predict)\n",
    "        return r2   \n",
    "print(\"mse\"+\"\\n\"+str(performance_metric(y_train, y_train)))\n",
    "\n",
    "def fit_model(X, y):\n",
    "        regressor = DecisionTreeRegressor()\n",
    "        parameters = {'max_depth':(1,2,3,4,5,6,7,8,9)}\n",
    "        scoring_function = make_scorer(performance_metric,\n",
    "         greater_is_better=True)\n",
    "        reg = GridSearchCV(regressor, parameters, scoring=scoring_function)\n",
    "        reg.fit(X,y)\n",
    "        print str(reg.best_estimator_)\n",
    "        return reg.best_estimator_\n",
    "\n",
    "print(\"Predicted sales prices for client input data:\"+\"\\n\"+str(fit_model(features, prices).predict(client_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicted sales prices for client input data:\n",
    ">>[ 408800, 231253.44827586, 938053.84615385]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <p>These numbers seem reasonable for the houses.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
