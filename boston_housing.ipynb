{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis\n",
    "   \n",
    "   <p>All requested statistics for the Boston Housing dataset are accurately calculated. Student correctly leverages NumPy functionality to obtain these results.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('housing.csv')\n",
    "prices = data['MEDV']\n",
    "features = data.drop('MEDV', axis = 1)\n",
    "\n",
    "print(\"Boston housing dataset has {} datapoints with {} variables each\".format(*data.shape))\n",
    "\n",
    "min_price = np.min(prices)\n",
    "print(\"Min:\"+\"\\n\"+str(min_price))\n",
    "\n",
    "max_price = np.max(prices)\n",
    "print(\"Max:\"+\"\\n\"+str(max_price))\n",
    "\n",
    "median_price = np.median(prices)\n",
    "print(\"Median:\"+\"\\n\"+str(median_price))\n",
    "\n",
    "std_price = np.std(prices)\n",
    "print(\"Standard Deviation:\"+\"\\n\"+str(std_price))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Feature Observation\n",
    "\n",
    "   \n",
    "   <p>Student correctly justifies how each feature correlates with an increase or decrease in the target variable.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer:\n",
    "\n",
    "   <p>Independent variables:\n",
    "    1) RM: Number of rooms in a house\n",
    "    2) LSTAT: Percentage of neighborhood population below poverty line.\n",
    "    3) PTR: Pupil-Teacher ratio\n",
    "    \n",
    "    Dependent variable:\n",
    "    1) Price of the house\n",
    "    \n",
    "    Correlations:\n",
    "    1) RM: Positively correlated to price. As RM goes up, price goes up.\n",
    "    2) LSTAT: Negatively correlated to price. As LSTAT goes up, price goes down.\n",
    "    3) PTR: Negatively correlated to price. As PTR goes up, price goes down.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - Goodness of Fit\n",
    "   \n",
    "   <p>\n",
    "    Assume that a dataset contains five data points and a model made the following predictions for the target variable:\n",
    "\n",
    "    True Values:[3.0, -0.5, 2.0, 7.0, 4.2]\t\n",
    "    Predictions:[2.5, 0.0, 2.1, 7.8, 5.3]\n",
    "        \n",
    "    Run the code cell below to use the performance_metric function and calculate this model's coefficient of determination.\n",
    "  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the performance of this model\n",
    "score = performance_metric([3, -0.5, 2.7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\n",
    "print \"Model has a coefficient of determination, R^2, of {:.3f}.\".format(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "   <p>Would you consider this model to have successfully captured the variation of the target variable? Why or why not?</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer:\n",
    "\n",
    "   <p>The r2_score function was imported from sklearn.metrics and the arrays of true and predicted values were plugged in. The resulting value calculated was 0.923.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "def performance_metric(y_true, y_predict):\n",
    "\terror = r2_score(y_true, y_predict)\n",
    "\treturn error\n",
    "print(\"r2_score\"+\"\\n\"+str(performance_metric(y_train, y_train)))\n",
    "print(str(performance_metric([3, -0.5, 2.7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])))\n",
    ">>> 0.923"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   <p>I would say this metric is pretty successful in capturating the variation of the target variable for this data. The value of 0.923 indicates a fairly good correlation between the true and predicted values. While the predictions are off by about 0.1-1.0, the predictions are consistently higher for the higher values and lower for the lower values.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3:\n",
    "   \n",
    "   <p>Student provides a valid reason for why a dataset is split into training and testing subsets for a model. Training and testing split is correctly implemented in code.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer:\n",
    "   \n",
    "   <p>Saving a portion of the dataset for testing allows us to evaluate the strength of our model and determine if we are overfitting or underfitting. If the model works well for the training data but poorly for the testing data, then the model overfits. If the model works well on the testing data but poorly on the training data, then the model underfits. \n",
    "    \n",
    "    Below, the code for splitting the dataset is shown: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "#Obtain the data\n",
    "data = pd.read_csv('housing.csv')\n",
    "prices = data['MEDV']\n",
    "features = data.drop('MEDV', axis = 1)\n",
    "#Split the data\n",
    "X_train, X_test, y_train, t_test = train_test_split(prices,\n",
    "\t\t\t\t\t\tfeatures,\n",
    "\t\t\t\t\t\ttest_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4:\n",
    "   \n",
    "   <p>Student correctly identifies the trend of both the training and testing curves from the graph as more training points are added. Discussion is made as to whether additional training points would benefit the model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "   \n",
    "   <p>Below, the learning curves prodcuded by the line 'vs.ModelLearning(features, prices)' in boston_housing.py are shown.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "   <p>Based on this these images, the best fit is the model with max_depth=3, because the score for the testing and training sets converge at a fairly high level. 150 data points is enough for the scores for the two sets to be nearly converged, and beyond 300 data points, the model does improve significantly.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "   \n",
    "   <p>Student correctly identifies whether the model at a max depth of 1 and a max depth of 10 suffer from either high bias or high variance, with justification using the complexity curves graph.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "\n",
    "   <p>The model with max_depth 10 always has a high score for the cross validation data, but never gets very good for the training data no matter how many data points are provided. This is a model with high variance/overfitting.\n",
    "    \n",
    "    The model with a max_depth of 1 never has a very high score for either the training or testing sets, but the two score values converge fairly quickly. This is a model with high bias/underfitting.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "   \n",
    "   <p>Student picks a best-guess optimal model with reasonable justification using the model complexity graph.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer:\n",
    "   \n",
    "   <p>The best model appears to be the model with max_depth=3, because the scores for the training and testing sets seem to converge at a fairly high value, using a reasonable number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "   \n",
    "   <p>Student correctly describes the grid search technique and how it can be applied to a learning algorithm.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "    \n",
    "   <p></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
